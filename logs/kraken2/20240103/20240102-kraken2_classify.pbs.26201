#!/usr/bin/env bash

# kraken2.pbs

# Run kraken2 in parallel

# Vivian Leung
# Created:       03 Jan 2024
# Last updated:  03 Jan 2024
# Last used:     03 Jan 2024

### Job name
### Queue name (qaussian): stdq1, medq1, fatq1 or gpuq1
### Wall time required (between 00:00:01 to 336:00:00).
### Number of CPUs: ppn: up to 40 for stdq1, 48 for gpuq1, 64 for medq1 and fatq1
### Memory: up to 750gb for stdq1, 1000gb for medq1 and gpuq1, 3000gb for fatq1

#PBS -N 20240102-kraken2_classify
#PBS -q stdq1
#PBS -l walltime=200:00:00
#PBS -l select=1:ncpus=40:mem=750gb

## Generally fixed settings
#__PBS -M vivian
#__PBS -m ae
#PBS -M leungvw@connect.hku.hk
#PBS -v MODULEPATH

### Declare job non-rerunable
#PBS -r n
#PBS -k oed

#######
set -e
set -o nounset
set -o errtrace
set -o pipefail

export start_time=$(date +'%s.%N') DRY_RUN= KEEP_TMP= NO_TMPDIR=

DRY_RUN='n'
KEEP_TMP='n'
NO_TMPDIR=1
# NO_TMPDIR=0  # don't make tmpdir   # for socket

############ SPECS AND PARAMS ############

# I/O

# constants exports for pbsutils and jobs
export PROJECT_DIR DATA_DIR INPUT_DIR OUT_DIR ROSTER
declare -A INPATHS OUT_DIRS OUTFILES

# key directories
PROJECT_DIR="/home/d24h_prog2/data_share/ice_water"
DATA_DIR="$PROJECT_DIR/data"
INPUT_DIR="$DATA_DIR/trimmed"

# # Headerless tsv with paired fastqs relpaths (relative to input dir)
ROSTER="$INPUT_DIR/trimmed_fqs.tsv"

export DBNAME=/home/d24h_prog2/PUBLIC_DB/kraken2/archaea_bacteria_et_al

# assoc array for easier logging
INPATHS=(
    [roster]="$ROSTER"
    [input_dir]="$INPUT_DIR"
    [dbname]="$DBNAME"
)

# Job output dir
OUT_DIR="${PROJECT_DIR}/jobs/${PBS_JOBNAME}-${PBS_JOBID%%.*}"

export SEQS_DIR="${OUT_DIR}/seqs"
export LOG_DIR="${OUT_DIR}/logs"

# output dirs to make
OUT_DIRS=(
    [main]="${OUT_DIR}"
    [classified]="$OUT_DIR"
    [seqs]="$SEQS_DIR"
    [log]="$LOG_DIR"
)

# environment settings
declare CONDA_ENV MODULES

CONDA_ENV="vivian"
MODULES=(
    'vivian/parallel/20221222'
    'vivian/kraken2/2.1.3'
)

# Exports: duration timestamp print_array print_params set_conda_env
#          DEFAULT_CONDA_PREFIX
# run /home/d24h_prog2/vivian/utils/utils.sh --help for helpdoc
source "/home/d24h_prog2/vivian/utils/utils.sh"


# GNU Parallel params
declare PARALLEL_JOBLOG
declare -a COMMON_PARALLEL_PARAMS COMMON_JOB_PARAMS
export THREADS_PER_JOB

# PARALLEL_JOBLOG="$DATA_DIR/joblog"
# INPATHS[joblog]="$PARALLEL_JOBLOG"

THREADS_PER_JOB=19

COMMON_PARALLEL_PARAMS=(
    --resume-failed
    --halt soon,fail=3
    --colsep $'\t'
    --jobs $(( (OMP_NUM_THREADS - 1) / THREADS_PER_JOB ))

)

# i/o flags and options on run
declare -a COMMON_JOB_ARGS=(
  'kraken2 --db "$DBNAME" --use-names --paired --threads "$THREADS_PER_JOB" --gzip-compressed --output "$(path_classified_out {1})" --classified-out "$(path_classified_seqs {1})" --unclassified-out "$(path_unclassified_seqs {1})" {1} {2} 2>"$(path_log {1})"'
)

  
############ UTILS AND FUNCTIONS  ############
# put things like path generation here

# fq_basename () { basename "$1" | sed -E 's/(_[12])?\.(fq|fastq)(\.gz)?$//' ; }
fq_basename () { basename "$1" | sed -E 's/(_[12])?(\.[^.]+)*\.(fq|fastq)(\.gz)?$//' ; }
export -f fq_basename

# File naming conventions

# usage: path_classified_out ACCESSION_OR_FPATH
path_classified_out () {
    echo "${OUT_DIR}/$(fq_basename "$1")#.classed.tsv"
}
export -f path_classified_out

# usage: path_classified_seqs ACCESSION_OR_FPATH
path_classified_seqs () {
    echo "${SEQS_DIR}/$(fq_basename "$1")#.classed.fq"
}
export -f path_classified_seqs

# usage: path_unclassified_seqs ACCESSION_OR_FPATH
path_unclassified_seqs () {
    echo "${SEQS_DIR}/$(fq_basename "$1")#.unclassed.fq"
}
export -f path_unclassified_seqs

# usage: path_unclassified_seqs ACCESSION_OR_FPATH
path_log () {
    echo "${LOG_DIR}/$(fq_basename "$1").log"
}
export -f path_log

# examples
OUTFILES+=(
  [kraken2_classification]="$(path_classified_out NAME)"
  [classified_seqs]="$(path_classified_seqs NAME)"
  [unclassified_seqs]="$(path_unclassified_seqs NAME)"
  [log]="$(path_log NAME)"
)

############  STANDARD/FIXED SETUP  ############

# Common utilities for PBS scripts

# Export PBS variables for pbsutils
export PBS_JOBNAME PBS_NODEFILE PBS_O_WORKDIR OMP_NUM_THREADS

# remove junk in PATH (manually put in by others in ~/.bashrc)
export PATH="$(clean_path "$PBS_O_PATH" -e '^\/home\/d24h_prog2\/' \
      -i '^\/home\/d24h_prog2\/(\.conda|miniconda3|vivian)\/')"

cd $PBS_O_WORKDIR

# Parse PBS variables

# export PBS variables for pbsutils
export NPROCS NNODES NCORES JID

NPROCS=$(wc -l < $PBS_NODEFILE)
NNODES=$(uniq $PBS_NODEFILE | wc -l)
NCORES=$((NPROCS / NNODES))
JID="${PBS_JOBID%%.*}"

# Parse input rosters and logs
export roster n_samples
if [[ -n "${ROSTER+foo}" ]] ; then
    roster="$PBS_O_WORKDIR/$PBS_JOBNAME.$(basename "$ROSTER").$JID"
    cp "$ROSTER" "$roster"
    n_samples=$(wc -l <"${roster}")
fi

# export for pbsutils
export joblog
joblog="$PBS_O_WORKDIR/$PBS_JOBNAME.joblog.$JID"

if [[ -n "${PARALLEL_JOBLOG:-}" ]] ; then
  echo "Creating copy of input job log $PARALLEL_JOBLOG..."
  cp "$PARALLEL_JOBLOG" "$joblog"
fi

# Save this pbs job script to jobs dir
cp "$0" "$PBS_O_WORKDIR/$PBS_JOBNAME.pbs.$JID"

##  Prep environment ##

# init conda env
set_conda_env "$CONDA_ENV"

# import modules
for mod in "${MODULES[@]}" ; do module load "${mod}" ; done

# this must come after defining all of the below exported
# exports: is_dry_run do_keep_tmp epilogue epilogue_simple trap_error
#          trap_finish_job standard_prologue io_prologue
source "/home/d24h_prog2/vivian/utils/pbsutils.sh"

trap trap_error ERR
trap epilogue EXIT

# print info about PBS and environment
standard_prologue

io_prologue 'INPATHS' 'OUT_DIRS' 'OUTFILES'

############  END OF STANDARD/FIXED SETUP  ############

# Report params and programs
cat <<-PROLOGUE_PROGRAMS_PARAMS

	============== Programs in Use ==============
	> GNU parallel : $([[ -n "$(which parallel)" ]] && parallel --version 2>&1)
	$(which parallel)
	$(which parcat)

	> kraken2:  $(kraken2 --version 2>&1)
	$(which kraken2)

	============ Common Command Params ============
	# > parallel:
	# $( (( ${#COMMON_PARALLEL_PARAMS[@]} > 0 )) &&  print_params "${COMMON_PARALLEL_PARAMS[@]}" )

	> kraken2
	$( (( ${#COMMON_JOB_ARGS[@]} > 0 )) &&  print_params "${COMMON_JOB_ARGS[@]}" )

	Threads per job:   ${THREADS_PER_JOB:-}

	Joblog: ${JOBLOG-$joblog}
PROLOGUE_PROGRAMS_PARAMS


####################  EXECUTE  ####################

printf "============== EXECUTING..... ==============\n"

# make output dirs
mkdir -p "${OUT_DIR}"
mkdir -pv "${OUT_DIRS[@]}"

# make tmpdir
if [[ ${NO_TMPDIR:-0} == 0  ]] ; then
  TMPDIR="$(mktemp -d)"
else
  mktempdir
# shorten path
  export TMPDIR="$(realpath --relative-to="${INPUT_DIR}" $TMPDIR)"
fi

# important: go to directory
cd "${INPUT_DIR}"

printf '%s Running kraken2\n' "$(timestamp)"

[[ -z "${n_samples:-}" ]] || printf '%s %d samples\n' "$(timestamp)" ${n_samples}


parallel "${COMMON_PARALLEL_PARAMS[@]}" --joblog "$joblog" \
    ${COMMON_JOB_ARGS[@]} :::: "$roster"
wait

